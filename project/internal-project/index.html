<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.4.0 for Hugo" />
  

  
  









  




  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Stephanie Weiss" />

  
  
  
    
  
  <meta name="description" content="An example of using the in-built project page." />

  
  <link rel="alternate" hreflang="en-us" href="https://Stephanie-Weiss.github.io/project/internal-project/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.512574799414e7d36471486881070b0f.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://Stephanie-Weiss.github.io/project/internal-project/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Academic" />
  <meta property="og:url" content="https://Stephanie-Weiss.github.io/project/internal-project/" />
  <meta property="og:title" content="Internal Project | Academic" />
  <meta property="og:description" content="An example of using the in-built project page." /><meta property="og:image" content="https://Stephanie-Weiss.github.io/project/internal-project/featured.jpg" />
    <meta property="twitter:image" content="https://Stephanie-Weiss.github.io/project/internal-project/featured.jpg" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2016-04-27T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2016-04-27T00:00:00&#43;00:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Stephanie-Weiss.github.io/project/internal-project/"
  },
  "headline": "Internal Project",
  
  "image": [
    "https://Stephanie-Weiss.github.io/project/internal-project/featured.jpg"
  ],
  
  "datePublished": "2016-04-27T00:00:00Z",
  "dateModified": "2016-04-27T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Stephanie Weiss"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Academic",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Stephanie-Weiss.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "An example of using the in-built project page."
}
</script>

  

  

  

  





  <title>Internal Project | Academic</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="8f66d660a9a2edc2d08e68cc30f701f7" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.13d3c1ccc66961c64e937f4e854fdd1e.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Academic</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Academic</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    <article class="article article-project">

  





















  
  


<div class="article-container pt-3">
  <h1>Internal Project</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2016
  </span>
  

  

  

  
  
  
  
  
  

  
  

</div>

  




<div class="btn-links mb-3">
  
  








  












  
  <a class="btn btn-outline-primary btn-page-header" href="/slides/example/" target="_blank">
    Slides
  </a>
  





  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://twitter.com/georgecushen" target="_blank" rel="noopener">
    <i class="fab fa-twitter mr-1"></i>Follow</a>


</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 405px;">
  <div style="position: relative">
    <img src="/project/internal-project/featured_hu3d03a01dcc18bc5be0e67db3d8d209a6_224363_720x2500_fit_q75_h2_lanczos.webp" width="720" height="405" alt="" class="featured-image">
    <span class="article-header-caption">Photo by rawpixel on Unsplash</span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Abstract</p>
<p>Owing to advancing digitalization, consumers have become more fastidious about the story ventures create about themselves. The increased demand regarding the aesthetical representation of storytelling has prompted many entrepreneurs to trust upon more sophisticated forms, such as images and videos, to create an identity with which they aim to appeal to consumers. For research in the field of storytelling, this step of modernization entails innovating the methodological approaches adopted to analyze visual storytelling. Only then, scholars are able to make reliable statements about the influence of visual storytelling on, e.g., sales tracking. The sales giant Amazon has taken a step in this direction by offering its platform-owned ML-web services. In the scope of this article, it is looked at the potential two of these provided services, Amazon Transcribe (Automatic speech recognition) and Amazon Rekognition (Image and video analysis), hold for advancing the research field of storytelling.</p>
<p>Figure 1: Photo by Kevin Ku on Unsplash</p>
<p>Storytelling – A Powerful, Well-Established Tool</p>
<p>Push messages with image ads pop up on our smartphone screens, video ads interrupt the series we are watching, Facebook &amp; Co. advertise goods we have searched for lately, surrounded by posts of our friends – for those occurrences not to drown in the daily advertisement noise we are experiencing but to make us either focus, sympathize, or laugh, it is important for marketers to master the art of storytelling. Recently, I have watched a very inspiring TEDx Talk by David J. P. Phillips, a man who is known as “Mr. Death by Power Point”, about how this can look like and what is the magic behind storytelling (<a href="https://www.youtube.com/watch?v=Nj-hdQMa3uA&amp;feature=youtu.be%29" target="_blank" rel="noopener">https://www.youtube.com/watch?v=Nj-hdQMa3uA&feature=youtu.be)</a>. Aside from him, there are numerous other supporters who attach a powerful role to storytelling (Figure 2).</p>
<p>Figure 2: Photo by Florian Klauer on Unsplash</p>
<p>Why storytelling should not be underrated and how ventures can benefit from an effective storytelling has been studied by several scholars (e.g., Lounsbury &amp; Glynn, 2001; Martens, Jennings, &amp; Devereaux Jennings, 2007; Navis &amp; Glynn, 2010). In this article, however, I argue that according to the fact that stories are increasing in complexity regarding the interplay of audio track, images, music, etc. (Pohlgeers, 2019), it is time for a methodological revolution in the field of narratives, e.g., to get a better understanding of the impact they have on sales success. By doing so, I aim to push forward and contribute to visual storytelling as under-researched field. For this purpose, I want to introduce innovative methods and discuss their potential as well as their weaknesses for the analysis of storytelling, contributing to modern entrepreneurship research.</p>
<p>Not only does storytelling work in an entrepreneurial context, but also scholars, to whom I count myself, have to tell a compelling story with their research papers in order to be published. I have been told once by a co-writer that there are three characteristics which make a paper “sexy” and increase its publication chances: (1) new conceptional ideas, (2) an interesting data set, or (3) an innovative method. On the hunt for meeting one of these criteria with one of my research projects, I searched for new avenues in the field of storytelling. During this search, I stumbled upon Amazon Web Service (AWS), a massive platform that provides powerful tools, amongst others, based on artificial intelligence and machine learning, opening up new possibilities and solutions in all kinds of fields, including academic research. Especially the two tools Amazon Rekognition and Amazon Transcribe caught my interest by evoking enthusiasm for new methodological routes I could take in my research field of storytelling. Amazon Rekognition, forming part of facial recognition technologies (FRT), e.g., detects faces, objects, labels, text, safe content, and celebrities in images or videos, and Amazon Transcribe, which converts audio into text, could both revolutionize storytelling research in general and in an entrepreneurial context in particular. By understanding how elements involved in storytelling such as narratives, visuals, and images serve as mechanism to form a venture’s identity (Gehman &amp; Soublière, 2017), researchers could, on the one hand, make valuable contributions to legitimacy literature; on the other hand, they could advise ventures how to create a compelling identity through storytelling in order to be perceived as legitimate by consumers (Martens et al., 2007).</p>
<p>Why Are New Methods Needed in The Field of Entrepreneurial Narratives?</p>
<p>To better account for the power storytelling holds and finding adequate ways to methodologically, grasp the complexity of this today highly mediatized tool, I claim that new methods are needed in the field of storytelling or rather the maturing process of recently emerging methods should be advanced. An emerging methodological revolution already becomes apparent when exploring existing literature in Table 1. In particular during the last two decades, I observe a development from conceptual approaches to natural language processing (NLP). While several studies have examined narrational storytelling, the field of visual storytelling is just starting to become more prominent.</p>
<p>Table 1: Literature review (1977-2020)
The reasons for new methods which advance research on visual storytelling to create untapped opportunities are twofold. First, innovative methods in this field provide new insights in the phenomenon of storytelling. Second, new combinations of different approaches arise with the aim of gaining a deeper understanding of storytelling and bring together a yet scattered research field (Lounsbury, Cornelissen, Granqvist, &amp; Grodal, 2018). One such combination of approaches could be to investigate the relationship between aesthetics and innovation to gain new insights into sociocultural trends (Jancsary, Meyer, Höllerer, &amp; Boxenbaum, 2018). By aesthetics, existing literature refers to a technological product’s design (Lounsbury et al., 2018). I argue, however, that this interpretation should be extended to a narrative’s design. Another avenue a study about modern entrepreneurship points at relates to research on storytelling as strategic means (Überbacher, 2014) and how multiple linguistic constructs interplay (Lounsbury et al., 2018). To understand the dynamics stemming from this interplay, they motivate research on exploring interactions between contextual factors, such as spaces, places, objects, and language, all factors which scholars could examine with Amazon Rekognition and Amazon Transcribe.</p>
<p>Moreover, I argue that what has already been established in video games user research (Mirza-Babaei, Long, Foley, &amp; McAllister, 2011) could also be promising to overcome the lack of metrics and evaluation methods to measure story effectiveness of new ventures, such as interest stimulation, memorability, and informativeness (Kosara &amp; MacKinlay, 2013) as well as capturing the interplay happening in storytelling. How methods such as video mining as current state of the art provides ways to advance business research in this field demonstrates a recent study in which scholars investigate the appeal of movie trailers (Schwenzow et al., 2020). Relying on biometric data generated through automated processes might, on the one hand, improve replicability of studies when compared to human rating (Chouinard, Scott, &amp; Cusack, 2019), on the other hand, research using machine learning-based algorithms might not be prone to subjectivity issues but instead to error rates. Thus, I assume, at least for an early phase of artificial intelligence (AI), that it might be promising to be a two-timer, combining AI with human crosscheck through, e.g. the use of Amazon’s Mechanical Turk (MTurk) (Chan, Parhankangas, Sahaym, &amp; Oo, 2019). This would allow for visualization studies which are not solely student-based (Kosara &amp; MacKinlay, 2013), increasing their chances of being accepted at journals such as Entrepreneurship Theory and Practice, which abstains from papers with student-based data samples unless the research question shows a clear relevance for consulting students.</p>
<p>Amazon Web Services (AWS) – a Golden Fleece or a Pandora’s Box?</p>
<p>A solely Amazon-based approach, on the other hand, might be a source of new, yet unforeseeable consequences. Hence, a question I am asking myself is whether AWS incarnates a Golden Fleece or a Pandora’s box with the big stirs Amazon as provider of the service platform is creating. These two concepts trace back to Greek mythology where The Golden Fleece is used as a symbol for something valuable with “magic power” and “an object of worship” (Lordkipanidze, 2001, p. 3) that needs to be protected for “whoever owned the Fleece could reign!” (Lordkipanidze, 2001, p. 4). As opposed to this, a Pandora’s box is a proverb we use to refer to “a source of unexpected troubles and pain” (Lordkipanidze, 2001, p. 33). From my point of view, AWS seems to perform a tightrope act with two forces pulling: tools leading to innovative solutions and improvements in diverse fields and a growing monopoly which is involved in a powerful surveillance system, and thus criticized for violating privacy rights, as well as for having momentous error rates and a biased algorithm (Blank, 2019). Before considering the tools provided by AWS for research purposes, I wanted to dig deeper and get an idea of how to estimate these three main concerns coming with the use of FRT in general and through an academic lens in particular.</p>
<p>Facial Data – Privacy Rights as Pitfall</p>
<p>FRT have already entered several areas of our everyday lives. When we transition from one country to another, a snapshot is taken at the airport. Booking an accommodation via Airbnb requires storing our ID card. With our Face ID, we can unlock our iPhone. Facebook uses its “photo tag suggest” tool to identify our friends. Nowadays, this enumeration of FRT application fields seems to be endless. Although these technologies have positive contributions, facilitating our everyday lives, with the advance of FRT also come concerns whether our privacy rights remain unviolated.</p>
<p>As an article by Whitener and Aragon (2019) shows privacy rights’ violation is an explosive issue, reflecting in the General Data Protection Regulation (GDPR) (EU) and the Biometric Information Privacy Act (BIPA) (US). Both regulations distinguish between facial geometry and simple photographs. While the GDPR demands an active consent from the individual whose biometric data should be processed for identification purposes, the BIPA is limited to a mere disclosure duty (Whitener and Aragon, 2019).</p>
<p>Figure 3: Photo by Nadine Shaabana on Unsplash
As a study has shown, people are willing to trade benefits for biometric privacy since they are not solely concerned by data security but also by its use (Kugler, 2019). Society strongly criticizes the use of FRT especially in the realm of law enforcement, such as border control and mass surveillance. “Border management is becoming increasingly datalogical” (Mattern, 2018), leading to dehumanizing. “The border-crosser becomes a pixelated subject[s] … composed of so many data points that the individual is no longer a meaningful category.” (Longo, 2017, p. 224). Also, political activists and protesters see this technology as danger (O’Neill, 2019).</p>
<p>The contested deployment of FRT and its potential for mistaken arrests (Fingas, 2018) induced over 100 Amazonians to issue a letter to Jeff Bezos, CEO of Amazon, asking to stop selling facial-recognition software to police and to start taking responsibility (Conger, 2018). The trigger for the request was the disclosure of the collaboration between Amazon, the government, and the police force by the non-profit organization The American Civil Liberties Union (ACLU), which is committed to protecting personal rights. The Amazonians saw a serious threat for marginalized groups through the use of Amazon Rekognition: “We already know that in the midst of historic militarization of police, renewed targeting of Black activists, and the growth of a federal deportation force currently engaged in human rights abuses — this will be another powerful tool for the surveillance state, and ultimately serve to harm the most marginalized” (Conger, 2018).</p>
<p>Male Whites as Standard – An Ongoing Legacy of Slavery and Colonial Times?</p>
<p>Not only is FRT, such as Amazon Rekognition, seen critically due to</p>
<p>Figure 4: Photo by Arshad Khan on Unsplash
violation of privacy rights, but also it raises concerns with regard to bias (Klare, Burge, Klontz, Bruegge, &amp; Jain, 2012). This brings up the question of the type of bias and who is responsible for it. “Algorithms themselves have no moral agency” (Blank, 2019, p. 6). As a consequence, they are only as good as the training data which their programmers feed them under the influence of their own cultural values. Does that imply that these algorithms do not provide improvements in terms of objectivity? Do they have to be deculturized first?</p>
<p>Since most algorithms, such as, e.g., Microsoft, IBM, Google, and Amazon, have their origin in diversity lacking, American companies (Levin, 2019), Western culture is dominant in AI, which, as results of these algorithms show, marginalizes certain groups, i.e., women, as well as darker-skinned humans, reminding us of slavery and colonial times (Laura &amp; Thomi, 2019; Magnet, 2011). As a study has shown, accuracy of APIs’ results decreases by the following order of training data origin: Caucasian, Indian, Asian and African (Wang, Deng, Hu, Tao, &amp; Huang, 2018). Low cultural distance between developers and the person to be identified improves outcomes of APIs (Beveridge, Givens, Phillips, Draper, &amp; Lui, 2008), supporting the hypothesis that the culture of an algorithm’s origin is dominating. Compared to lighter-skinned males, for darker-skinned females error rates for gender misclassification are up to 34.4% (Buolamwini &amp; Gebru, 2018). This high error rate even led to a classification of African American faces as gorillas by Google’s algorithm (Simonite, 2018).</p>
<p>Figure 5: Photo by Prince Akachi on Unsplash</p>
<p>Figure 6: Photo by Rob Tol on Unsplash</p>
<p>As a study has shown, FRT are not only racist and sexist (Buolamwini &amp; Gebru, 2018) but also age-discriminatory, indicated by an underrepresentation of underaged and elderly (Anda, Lillis, Le-Khac, &amp; Scanlon, 2018). Hence, “biometric technologies not only fail to live up to promises of objectivity and reliability, but in fact continue a long history of misrecognizing women, people of color, and people with disabilities” (Mattern, 2018).</p>
<p>Are Amazon Transcribe and Amazon Rekognition Useful and Reliable?</p>
<p>Apart from concerns regarding privacy rights and bias, a key question that will determine the future of services such as Amazon Rekognition, especially when used for academic purposes, relates to their reliability. To test the usefulness and reliability of Amazon Transcribe and Amazon Rekognition and to exemplify how these tools could revolutionize current methodological approaches in storytelling, I, owing to an own interest in sustainable entrepreneurship, analyzed the following front video of a Kickstarter campaign (alternatively you can go to <a href="https://www.youtube.com/watch?v=CxBoTMWQTd0" target="_blank" rel="noopener">https://www.youtube.com/watch?v=CxBoTMWQTd0</a> where only the Kickstarter dedication is missing). The video starts with a waterfall scenery representing the massive amount of water we use for our daily showers. Throughout the video four different subjects (1 man, 2 women, 1 girl) test the innovative, water-saving shower the company produces. The viewer learns about the journey of the company starting from the idea, going through a developmental design and engineering process, resulting in the final product of which water efficiency is demonstrated by the filling levels of two aquariums representing the water consumption during a “normal” shower compared to when using the innovative product. The story about the product’s journey is accompanied by meeting scenes in office rooms covered with sketches and close-ups of the product. Crowdfunding is an interesting environment to investigate visual storytelling since campaigns with a video are much more likely to reach their funding goals, whereas those without a video have a hard time receiving sufficient funding (Kickstarter, 2020).</p>
<p>Amazon Transcribe – from Speech to Text</p>
<p>For a reliability check-up of Amazon Transcribe, first, I closely scrutinized the rendered transcript by comparing it to the original audio track in the sample video. Figure 7 displays the transcript with incorrectly transcribed parts which I emphasized in different colors and added corrections. With the help of these emphases, I evaluate total accuracy, group error types, and discuss consequences for academic analyses. In total, Amazon Transcribe shows an accuracy of 94,92% (incorrectly transcribes 33 out of 649 words (excluding sentence transition issues). Only 73 words have a confidence level below 85%. 26 of these 73 words (35,62%) are actually incorrectly transcribed. The different colors in Figure 7 represent different badness scales. While red-marked parts severely impact the transcript’s meaning (16), yellow-marked parts emphasize minor modifications (14), concerning mainly sentence-transition. I used brownish color to emphasize sample-specific errors, relating to the product’s name (10). I am pleasantly surprised how accurate Amazon Transcribe worked. When assigning the right weight to the confidence values, using Amazon Transcribe might facilitate investigating narratives embedded in videos in the future. Only analyses based on sentence length might be problematic due to the frequent errors occurring with sentence transition. Moreover, accuracy should be tested for different accents since Amazon Transcribe might have problems transcribing non-standard English.</p>
<p>Figure 7: Output of Amazon Transcribe
After exploring Amazon Transcribe and its reliability, I turn to Amazon Rekognition. In the scope of this article, I focus on detecting faces and detecting labels as two provided commands. Due to copyright reasons, I took two random pictures (Figure 8 and Figure 9) that could fit the sample crowdfunding campaign I am using to demonstrate what can be done with the tools of interest provided by AWS. For detecting faces, a bounding box with landmarks is created. Based on this face measurement, age, gender, emotions, and other characteristics can be determined. Further information is given about the quality regarding brightness and sharpness as well as the face rotation.</p>
<p>Table 2: Detect faces output for Figure 7</p>
<p>Table 2: Detect faces output for Figure 7</p>
<p>Figure 8: Photo by The Creative Exchange on Unsplash</p>
<p>Table 3: Detect faces output for Figure 8</p>
<p>Figure 9: Photo by Christian buehner on Unsplash</p>
<p>For Figure 8 and Figure 9, results are for the most part reliable. Concerning age, I notice that with increasing age, the age range Amazon Rekognition gives seems to widen. This is in line with prior findings that Amazon Rekognition performs best “for all ranges up to the age of 29” (Anda et al., 2018, p.136).  The mean absolute age error estimation is 9.286. To exemplify this, for a woman at the age of 54, the system might render a result of 45. Error rates for males are lower (Anda et al., 2018). Amazon Rekognition’s perfomance for estimating humans at younger ages may already open up new ways to investigate the impact of infant behavior in visual storytelling (Chouinard et al., 2019). Also, results for gender correspond to the accuracy rates identified in recent studies (Jung, An, Kwak, Salminen, &amp; Jansen, 2018; Yavuz, 2019). For emotions, Amazon Rekognition correctly considered happy and calm to be the dominant emotions in Figure 7 and 8 respectively. Being able to identify emotions in pictures will open up new possibilities to advance the field of collective indexing of emotions (Knautz and Stock, 2011; Reyes and Bahm, 2016). Only the confidence level for beard in Figure 8 is surprisingly low.</p>
<p>Amazon Rekognition – Detecting Labels</p>
<p>For the sample video, Amazon Rekognition identifies 4640 labels. These labels refer to detected objects, scenes, and activities. Using the statistical software R, I requested the number of unique labels, which rendered a list of 288 unique labels, as displayed in Figure 10. 20,83% of the labels were incorrectly recognized. It stands out that 8,3% of the error labels result from misinterpreting the white bathroom gown which is worn by the woman who is showering in the video. Another 10% of the error labels represent situations in which many people come together. This assumption is based on the team meetings displayed in the video.</p>
<p>Even though having a lower accuracy rate than Amazon Transcribe, Amazon Rekognition’s tool for detecting labels still convinced me of its usefulness for future research since most of the labels very well described the overall visual content of the video. Knowing the unique labels used in visual storytelling opens up new avenues to apply topic modeling and to explore interrelations between labels themselves as well as in comparison to spoken and/or written narratives. Since the labels come with start and end times, it is also possible to get a deeper understanding of how timing in terms of label detection impacts a video’s attractiveness.</p>
<p>Figure 10: List of unique labels
Looking into the Crystal Ball – What the Future May Hold for FRT in General and For Academia in Particular</p>
<p>Accuracy rates are of central importance, especially in fields where based on FRT momentous decisions are made, such as prison sentence or even death penalty. This real threat imposed by FRT, such as Amazon Rekognition, has become omnipresent by the American Civil Liberties Union (ACLU) congress scandal at the latest, where 28 Congress members were falsely matched with persons involved in criminal actions grounded on a confidence level of 80% (Brandom, 2018). ACLU questioned the use of FRT in law enforcement (Snow, 2018). Matt Wood, general manager of AI at AWS, countered with arguing that the confidence level should be set to 90% for public safety issues (Wood, 2018b).</p>
<p>What should be done with a tool that, on the one hand, is able to prevent “human trafficking, inhibiting child exploitation, and reuniting missing children with their families” (Wood, 2018a); on the other hand, it violates privacy rights, discriminates marginalized groups, and has the power to send an innocent person to jail? AWS suggests that to benefit from this powerful tool society needs to prevent misuse of this technology (Wood, 2018b). To ensure this, an established “ethical infrastructure” is required (Floridi, 2013). Otherwise, mistrust might cast a shadow on FRT leverage, resulting in untapped opportunities for improvement, such as using FRT for recognizing objects and alert visually impaired if they become an obstacle for them (Tripathi, Halder, &amp; Vanusha, 2018).</p>
<p>To regulate FRTs’ great power, government, technological companies, and society have to work hand in hand (Blank, 2019). With the help of such a “distributed agency” (Taddeo &amp; Floridi, 2018), regulation is possible. According to Zeng et al. (2019), a responsible use of FRT includes six steps: (1) Improve data privacy, (2) improve accuracy and robustness, (3) reduce bias, (4) inform society, (5) create encrypted test data, (6) constantly check for ethical risks. These steps are congruent with the EU’s AI check list. First movements into this direction have been made in form of the GDPR and the BIPA. There are also first cases were bans and fine issues of FRT were issued (Dave Lee, 2019; Edvardsen, 2019; Fisher, 2019). How the distributed agency ensures the right use of FRT only time will tell. For FRT research not to become “dangerous” and proverbially “deadly”, what Durrenmatt already warned of, also we as researchers have the responsibility to deal with data wisely and should be aware that the use of tools such as Amazon Transcribe and Amazon Rekognition not only processes data but also generates new ones.</p>
<p>For using FRT for academic purposes, I can stress that automating processes through algorithms will facilitate data analysis, especially for larger data sets. On an individual level, errors caused by algorithms can have tremendous consequences, research based on large data sets, however, can prevent a serious distortion of results with the help of confidence values, robustness checks, and random human cross-check, at least in an early stage of machine learning. I believe that when having ensured reliability, services such as Amazon Transcribe and Amazon Rekognition have the power to revolutionize and modernize storytelling research by enabling new approaches which are capable of examining the complex interplay happening in storytelling.</p>
<p>Overall, I think that retelling the methodological story in the field of visual storytelling is of relevance to three groups. Researchers can profit from using innovative methods in order to get a deeper understanding of the power of visual storytelling and learn about how to combine different methodological approaches. Moreover, new insights gained from the use of these innovative methods may help organizations in the future to enhance their performance by improving visual narratives. As a positive side-effect, society may profit from an improved shopping experience.</p>
<p>References</p>
<p>Allison, T. H., Davis, B. C., Short, J. C., &amp; Webb, J. W. (2014). Crowdfunding in a prosocial microlending environment: Examining the role of intrinsic versus extrinsic cues. Entrepreneurship: Theory and Practice, 39(1), 53–73. <a href="https://doi.org/10.1111/etap.12108" target="_blank" rel="noopener">https://doi.org/10.1111/etap.12108</a></p>
<p>Allison, T. H., McKenny, A. F., &amp; Short, J. C. (2013). The effect of entrepreneurial rhetoric on microlending investment: An examination of the warm-glow effect. Journal of Business Venturing, 28(6), 690–707. <a href="https://doi.org/10.1016/j.jbusvent.2013.01.003" target="_blank" rel="noopener">https://doi.org/10.1016/j.jbusvent.2013.01.003</a></p>
<p>Anda, F., Lillis, D., Le-Khac, N. A., &amp; Scanlon, M. (2018). Evaluating automated facial age estimation techniques for digital forensics. Proceedings – 2018 IEEE Symposium on Security and Privacy Workshops, SPW 2018, 129–139. <a href="https://doi.org/10.1109/SPW.2018.00028" target="_blank" rel="noopener">https://doi.org/10.1109/SPW.2018.00028</a></p>
<p>Beveridge, J. R., Givens, G. H., Phillips, P. J., Draper, B. A., &amp; Lui, Y. M. (2008). Focus on quality, predicting frvt 2006 performance. IEEE International Conference, 1–8.</p>
<p>Blank, A. L. (2019). Computer Vision Machine Learning and Future- Oriented Ethics.</p>
<p>Brandom, R. (2018). Amazon’s facial recognition matched 28 members of Congress to criminal mugshots. Retrieved from <a href="https://www.theverge.com/2018/7/26/17615634/amazon-rekognition-aclu-mug-shot-congress-facial-recognition" target="_blank" rel="noopener">https://www.theverge.com/2018/7/26/17615634/amazon-rekognition-aclu-mug-shot-congress-facial-recognition</a></p>
<p>Buolamwini, J., &amp; Gebru, T. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. Proceedings of Machine Learning Research, 81, 1–15.</p>
<p>Chan, C. S. R., Parhankangas, A., Sahaym, A., &amp; Oo, P. (2019). Bellwether and the herd? Unpacking the u-shaped relationship between prior funding and subsequent contributions in reward-based crowdfunding. Journal of Business Venturing, (April), 1–24. <a href="https://doi.org/10.1016/j.jbusvent.2019.04.002" target="_blank" rel="noopener">https://doi.org/10.1016/j.jbusvent.2019.04.002</a></p>
<p>Chouinard, B., Scott, K., &amp; Cusack, R. (2019). Using automatic face analysis to score infant behaviour from video collected online. Infant Behavior and Development, 54(May 2018), 1–12. <a href="https://doi.org/10.1016/j.infbeh.2018.11.004" target="_blank" rel="noopener">https://doi.org/10.1016/j.infbeh.2018.11.004</a></p>
<p>Clarke, J. (2011). Revitalizing Entrepreneurship: How Visual Symbols are Used in Entrepreneurial Performances. Journal of Management Studies, 48(6), 1365–1391. <a href="https://doi.org/10.1111/j.1467-6486.2010.01002.x" target="_blank" rel="noopener">https://doi.org/10.1111/j.1467-6486.2010.01002.x</a></p>
<p>Conger, K. (2018). Amazon Workers Demand Jeff Bezos Cancel Face Recognition Contracts With Law Enforcement. Retrieved from <a href="https://gizmodo.com/1827071904#" target="_blank" rel="noopener">https://gizmodo.com/1827071904#</a>!</p>
<p>Dave Lee, L. (2019). San Francisco Bans Facial Recognition in US First. Retrieved from <a href="https://www.bbc.com/news/technology-48276660" target="_blank" rel="noopener">www.bbc.com/news/technology-48276660</a></p>
<p>Davis, B. C., Hmieleski, K. M., Webb, J. W., &amp; Coombs, J. E. (2017). Funders’ positive affective reactions to entrepreneurs’ crowdfunding pitches: The influence of perceived product creativity and entrepreneurial passion. Journal of Business Venturing, 32(1), 90–106. <a href="https://doi.org/10.1016/j.jbusvent.2016.10.006" target="_blank" rel="noopener">https://doi.org/10.1016/j.jbusvent.2016.10.006</a></p>
<p>Edvardsen, S. (2019). How to Interpret Sweden’s First GDPR Fine on Facial Recognition in School. Retrieved from iapp.org/news/a/how-to-interpret-swedens-first-gdpr-fine-on- facial-recognition-in-school/</p>
<p>Escalas, J. E. (2004). Imagine Yourself in the Product : Mental Simulation , Narrative Transportation , and Persuasion. Journal of Advertising, 33(2), 37–48.</p>
<p>Fingas, J. (2018). Police Face Recognition Misidentified 2,300 as Potential CriminalsEngadget. Retrieved from <a href="https://www.engadget.com/2018/05/06/police-face-recognition-" target="_blank" rel="noopener">https://www.engadget.com/2018/05/06/police-face-recognition-</a> misidentified-2300-as-criminals/</p>
<p>Fisher, C. (2019). Oakland Bans City Use of Facial Recognition Software. Retrieved from <a href="https://www.engadget.com/2019/07/17/oakland-california-facial-recognition-ban/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAJS9qvqVURBnqMcqWMh86nAuwExGTWYKwfXo7L39UmZL-8pThjwuDzgGLv4XnmSn8-3_Mpaugef0ahbJGurHEGqMvKfOj92oRHiRJTg9D9zjYpWmKsfHVL5Mqnq2f3gJ_VIgjmlF9S-xSh0xfeFD_Pp8V9hBluysCMHdPKXZJ8pn" target="_blank" rel="noopener">https://www.engadget.com/2019/07/17/oakland-california-facial-recognition-ban/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAJS9qvqVURBnqMcqWMh86nAuwExGTWYKwfXo7L39UmZL-8pThjwuDzgGLv4XnmSn8-3_Mpaugef0ahbJGurHEGqMvKfOj92oRHiRJTg9D9zjYpWmKsfHVL5Mqnq2f3gJ_VIgjmlF9S-xSh0xfeFD_Pp8V9hBluysCMHdPKXZJ8pn</a></p>
<p>Floridi, L. (2013). Distrubted Moraility in an Information Society. Science and Engineering Ethics, 19(3), 727–743.</p>
<p>Garud, R., Schildt, H. A., &amp; Lant, T. K. (2014). Entrepreneurial Storytelling, Future Expectations, and the Paradox of Legitimacy. Organization Science, 25(5), 1479–1492. <a href="https://doi.org/10.1287/orsc.2014.0915" target="_blank" rel="noopener">https://doi.org/10.1287/orsc.2014.0915</a></p>
<p>Gehman, J., &amp; Soublière, J. F. (2017). Cultural entrepreneurship: from making culture to cultural making. Innovation: Management, Policy and Practice, 19(1), 61–73. <a href="https://doi.org/10.1080/14479338.2016.1268521" target="_blank" rel="noopener">https://doi.org/10.1080/14479338.2016.1268521</a></p>
<p>Golant, B. D., &amp; Sillince, J. A. A. (2007). The Constitution of Organizational Legitimacy: A Narrative Perspective. Organization Studies, 28(8), 1149–1167. <a href="https://doi.org/10.1177/0170840607075671" target="_blank" rel="noopener">https://doi.org/10.1177/0170840607075671</a></p>
<p>Hanjalic, A., &amp; Xu, L. Q. (2005). Affective video content representation and modeling. IEEE Transactions on Multimedia, 7(1), 143–154. <a href="https://doi.org/10.1109/TMM.2004.840618" target="_blank" rel="noopener">https://doi.org/10.1109/TMM.2004.840618</a></p>
<p>Jancsary, D., Meyer, R., Höllerer, M. A., &amp; Boxenbaum, E. (2018). Institutions as multimodal accomplishments: Towards the analysis of visual registers. Research in the Sociology of Organizations, 54A, 87–117.</p>
<p>Jung, S. G., An, J., Kwak, H., Salminen, J., &amp; Jansen, B. J. (2018). Assessing the accuracy of four popular face recognition tools for inferring gender, age, and race. 12th International AAAI Conference on Web and Social Media, ICWSM 2018, (Icwsm), 624–627.</p>
<p>Kaminski, J., Jiang, Y., Piller, F., &amp; Hopp, C. (2017). Do user entrepreneurs speak different? Applying natural language processing to crowdfunding videos. Conference on Human Factors in Computing Systems – Proceedings, 2683–2689. <a href="https://doi.org/10.1145/3027063.3053223" target="_blank" rel="noopener">https://doi.org/10.1145/3027063.3053223</a></p>
<p>Kickstarter. (2020). How to make an awesome video. Retrieved from <a href="https://www.kickstarter.com/blog/how-to-make-an-awesome-video" target="_blank" rel="noopener">https://www.kickstarter.com/blog/how-to-make-an-awesome-video</a></p>
<p>Klare, B., Burge, M., Klontz, J., Bruegge, R., &amp; Jain, A. (2012). Face Recognition Performance: Role of Demographic Information. IEEE Transactions on Information Forensics and Security, 7(6). <a href="https://doi.org/10.1109/TIFS.2012.2214212" target="_blank" rel="noopener">https://doi.org/10.1109/TIFS.2012.2214212</a></p>
<p>Knautz, K., &amp; Stock, W. G. (2011). Collective indexing of emotions in videos. Journal of Documentation, 67(6), 975–994. <a href="https://doi.org/10.1108/00220411111183555" target="_blank" rel="noopener">https://doi.org/10.1108/00220411111183555</a></p>
<p>Kosara, R., &amp; MacKinlay, J. (2013). Storytelling: The next step for visualization. Computer, 46(5), 44–50. <a href="https://doi.org/10.1109/MC.2013.36" target="_blank" rel="noopener">https://doi.org/10.1109/MC.2013.36</a></p>
<p>Kugler, M. B. (2019). From Identification to Identity Theft: Public Perceptions of Biometric Privacy Harms. SSRN Electronic Journal. <a href="https://doi.org/10.2139/ssrn.3289850" target="_blank" rel="noopener">https://doi.org/10.2139/ssrn.3289850</a></p>
<p>Laura, A., &amp; Thomi, B. (2019). European Master ’ s Degree in Human Rights and Democratisation The Impact of Biometric Systems at EU Outside Borders on the Human Rights of Irregular Migrating Women.</p>
<p>Levin, S. (2019). “Bias deep inside the code”: the problem with AI “ethics” in Silicon Valley. Retrieved from <a href="https://www.theguardian.com/technology/2019/mar/28/big-tech-ai-ethics-" target="_blank" rel="noopener">www.theguardian.com/technology/2019/mar/28/big-tech-ai-ethics-</a> boards-prejudice</p>
<p>Longo, M. (2017). The Politics of Borders: Sovereignty, Security, and the Citizen after 9/11. Cambridge University Press.</p>
<p>Lordkipanidze, O. (2001). The Golden Fleece: Myth, Euhemeristic Explanation and Archaeology. Oxford Journal of Archaeology, 20(1), 1–38. <a href="https://doi.org/10.1111/1468-0092.00121" target="_blank" rel="noopener">https://doi.org/10.1111/1468-0092.00121</a></p>
<p>Lounsbury, M., Cornelissen, J., Granqvist, N., &amp; Grodal, S. (2018). Culture, innovation and entrepreneurship. Innovation: Management, Policy and Practice, 21(1), 1–12. <a href="https://doi.org/10.1080/14479338.2018.1537716" target="_blank" rel="noopener">https://doi.org/10.1080/14479338.2018.1537716</a></p>
<p>Lounsbury, M., &amp; Glynn, M. A. (2001). Cultural entrepreneurship: Stories, legitimacy, and the acquisition of resources. Strategic Management Journal, 22(6–7), 545–564. <a href="https://doi.org/10.1002/smj.188" target="_blank" rel="noopener">https://doi.org/10.1002/smj.188</a></p>
<p>Magnet, S. A. (2011). When Biometrics Fail: Gender, Race, and the Technology of Identity. Duke University Press.</p>
<p>Manning, S., &amp; Bejarano, T. A. (2017). Convincing the crowd: Entrepreneurial storytelling in crowdfunding campaigns. Strategic Organization, 15(2), 194–219. <a href="https://doi.org/10.1177/1476127016648500" target="_blank" rel="noopener">https://doi.org/10.1177/1476127016648500</a></p>
<p>Martens, M. L., Jennings, J. E., &amp; Devereaux Jennings, P. (2007). Do the Stories They Tell Get Them the Money They Need? The Role of Entrepreneurial Narratives in Resource Acquisition. Academy of Management Journal, 50(5), 1107–1132.</p>
<p>Mattern, S. (2018). All Eyes on the Border. Places Journal. <a href="https://doi.org/10.22269/180925" target="_blank" rel="noopener">https://doi.org/10.22269/180925</a></p>
<p>Mirza-Babaei, P., Long, S., Foley, E., &amp; McAllister, G. (2011). Understanding the contribution of biometrics to games user research. Proceedings of DiGRA 2011 Conference: Think Design Play, (October).</p>
<p>Mitra, T., &amp; Gilbert, E. (2014). The language that gets people to give. CSCW, 49–61. <a href="https://doi.org/10.1145/2531602.2531656" target="_blank" rel="noopener">https://doi.org/10.1145/2531602.2531656</a></p>
<p>Moss, T. W., Renko, M., Block, E., &amp; Meyskens, M. (2017). Funding the story of hybrid ventures: Crowdfunder lending preferences and linguistic hybridity. Journal of Business Venturing, 33(5), 643–659. <a href="https://doi.org/10.1016/j.jbusvent.2017.12.004" target="_blank" rel="noopener">https://doi.org/10.1016/j.jbusvent.2017.12.004</a></p>
<p>Navis, C., &amp; Glynn, M. A. (2010). How New Market Categories Emerge : Radio Satellite. Administrative Science Quarterly, 55, 439–471.</p>
<p>Navis, C., &amp; Glynn, M. A. (2011). Legitimate distinctiveness and the entrepreneurial identity: Influence on investor judgments of new venture plausibility. Academy of Management Review, 36(3), 479–499.</p>
<p>O’Neill, K. (2019). “Facebook’s ‘10 Year Challenge’ Is Just a Harmless Meme- Right?”</p>
<p>Parhankangas, A., &amp; Renko, M. (2017). Linguistic style and crowdfunding success among social and commercial entrepreneurs. Journal of Business Venturing, 32(2), 215–236. <a href="https://doi.org/10.1016/j.jbusvent.2016.11.001" target="_blank" rel="noopener">https://doi.org/10.1016/j.jbusvent.2016.11.001</a></p>
<p>Pollack, J. M., Rutherford, M. W., &amp; Nagy, B. G. (2012). Preparedness and cognitive legitimacy as antecedents of new venture funding in televised business pitches. Entrepreneurship: Theory and Practice, 36(5), 915–939. <a href="https://doi.org/10.1111/j.1540-6520.2012.00531.x" target="_blank" rel="noopener">https://doi.org/10.1111/j.1540-6520.2012.00531.x</a></p>
<p>Resnik, A., &amp; Stern, B. L. (1977). An Analysis of Information Content in Television Advertising. Journal of Marketing, 41(1), 50–53.</p>
<p>Reyes, J., &amp; Bahm, C. R. (2016). Crowdfunding: Applying collective indexing of emotions to campaign videos. Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW, 26-Februar, 385–388. <a href="https://doi.org/10.1145/2818052.2869075" target="_blank" rel="noopener">https://doi.org/10.1145/2818052.2869075</a></p>
<p>Roundy, P. (2009). Doing good by telling stories: emotion in social entrepreneurship communication. Journal of Small Business Strategy, 19(2), 41–69.</p>
<p>Sahouria, E., &amp; Avideh, Z. (1999). Content Analysis of Video Using Principal Components. Journal of the American Statistical Association, 9(8), 1290–1298. <a href="https://doi.org/10.1080/01621459.1936.10503354" target="_blank" rel="noopener">https://doi.org/10.1080/01621459.1936.10503354</a></p>
<p>Schwenzow, J., Hartmann, J., Schikowsky, A., &amp; Heitmann, M. (2020). Understanding Videos at Scale : How to Extract Insights for Business Research.</p>
<p>Simonite, T. (2018). When It Comes to Gorillas, Google Photos Remains Blind. Retrieved from <a href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/" target="_blank" rel="noopener">https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/</a></p>
<p>Snow, J. (2018). Amazon’s Face Recognition Falsely Matched 28 Members of Congress With Mugshots. ACLU. Retrieved from <a href="https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28" target="_blank" rel="noopener">https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28</a></p>
<p>Taddeo, M., &amp; Floridi, L. (2018). How AI Can Be a Force for Good. Science, 361(6404), 751–752.</p>
<p>Tripathi, S., Halder, C., &amp; Vanusha, D. (2018). A Survey on Assistive Technology for the Visually Impaired. International Research Journal of Engineering and Technology (IRJET), 1152–1155.</p>
<p>Überbacher, F. (2014). Legitimation of New Ventures: A Review and Research Programme. Journal of Management Studies, 51(4), 667–698. <a href="https://doi.org/10.1111/joms.12077" target="_blank" rel="noopener">https://doi.org/10.1111/joms.12077</a></p>
<p>van Werven, R., Bouwmeester, O., &amp; Cornelissen, J. P. (2015). The power of arguments: How entrepreneurs convince stakeholders of the legitimate distinctiveness of their ventures. Journal of Business Venturing, 30(4), 616–631. <a href="https://doi.org/10.1016/j.jbusvent.2014.08.001" target="_blank" rel="noopener">https://doi.org/10.1016/j.jbusvent.2014.08.001</a></p>
<p>Wang, M., Deng, W., Hu, J., Tao, X., &amp; Huang, Y. (2018). Racial Faces in-the-Wild: Reducing Racial Bias by Information Maximization Adaptation Network. Retrieved from <a href="http://arxiv.org/abs/1812.00194" target="_blank" rel="noopener">http://arxiv.org/abs/1812.00194</a></p>
<p>Wehnert, P., Baccarella, C. V., &amp; Beckmann, M. (2019). In crowdfunding we trust? Investigating crowdfunding success as a signal for enhancing trust in sustainable product features. Technological Forecasting and Social Change, 141, 128–137. <a href="https://doi.org/10.1016/j.techfore.2018.06.036" target="_blank" rel="noopener">https://doi.org/10.1016/j.techfore.2018.06.036</a></p>
<p>Whitener, M., &amp; Aragon, R. (2019). How should we regulate facial-recognition technology?</p>
<p>Wood, M. (2018a). Amazon general manager of artificial intelligence highlights positive uses of Rekognition &amp; acceptable use policy. Business &amp; Human Rights Resource Center. Retrieved from <a href="https://www.business-humanrights.org/en/amazon-general-manager-of-artificial-intelligence-highlights-positive-uses-of-rekognition-acceptable-use-policy" target="_blank" rel="noopener">https://www.business-humanrights.org/en/amazon-general-manager-of-artificial-intelligence-highlights-positive-uses-of-rekognition-acceptable-use-policy</a></p>
<p>Wood, M. (2018b). Thoughts On Machine Learning Accuracy. Retrieved from <a href="https://aws.amazon.com/blogs/aws/thoughts-on-machine-learning-accuracy/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/aws/thoughts-on-machine-learning-accuracy/</a></p>
<p>Wry, T., Lounsbury, M., &amp; Glynn, M. A. (2011). Legitimating Nascent Collective Identities: Coordinating Cultural Entrepreneurship. Organization Science, 22(2), 449–463. <a href="https://doi.org/10.1287/orsc.1100.0613" target="_blank" rel="noopener">https://doi.org/10.1287/orsc.1100.0613</a></p>
<p>Xu, P., Chen, L., &amp; Santhanam, R. (2015). Will video be the next generation of e-commerce product reviews? Presentation format and the role of product type. Decision Support Systems, 73, 85–96. <a href="https://doi.org/10.1016/j.dss.2015.03.001" target="_blank" rel="noopener">https://doi.org/10.1016/j.dss.2015.03.001</a></p>
<p>Yang, S., Kher, R., &amp; Newbert, S. L. (2019). What signals matter for social startups? It depends: The influence of gender role congruity on social impact accelerator selection decisions. Journal of Business Venturing, (March). <a href="https://doi.org/10.1016/j.jbusvent.2019.03.001" target="_blank" rel="noopener">https://doi.org/10.1016/j.jbusvent.2019.03.001</a></p>
<p>Yavuz, A. (2019). Evaluierung cloudbasierter Machine Learning Services, 49. Retrieved from <a href="http://edoc.sub.uni-hamburg.de/haw/volltexte/2019/4531/pdf/Thesis.pdf" target="_blank" rel="noopener">http://edoc.sub.uni-hamburg.de/haw/volltexte/2019/4531/pdf/Thesis.pdf</a></p>
<p>Zeng, Y., Lu, E., Sun, Y., &amp; Tian, R. (2019). Responsible Facial Recognition and Beyond. Retrieved from <a href="https://arxiv.org/pdf/1909.12935.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1909.12935.pdf</a></p>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/deep-learning/">Deep Learning</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://Stephanie-Weiss.github.io/project/internal-project/&amp;text=Internal%20Project" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://Stephanie-Weiss.github.io/project/internal-project/&amp;t=Internal%20Project" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Internal%20Project&amp;body=https://Stephanie-Weiss.github.io/project/internal-project/" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://Stephanie-Weiss.github.io/project/internal-project/&amp;title=Internal%20Project" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Internal%20Project%20https://Stephanie-Weiss.github.io/project/internal-project/" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://Stephanie-Weiss.github.io/project/internal-project/&amp;title=Internal%20Project" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://Stephanie-Weiss.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/stephanie-weiss/avatar_hu5ccf5e556d93c2625aef45b111ad9c0c_49041_270x270_fill_q75_lanczos_center.jpg" alt="Stephanie Weiss"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://Stephanie-Weiss.github.io/">Stephanie Weiss</a></h5>
      <h6 class="card-subtitle">PhD student of Entrepreneurship in Context</h6>
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.sme-gradschool.wiwi.uni-siegen.de/team/researchers/stephanie-weiss/" target="_blank" rel="noopener">
        <i class="fas fa-graduation-cap"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/stephanie-weiss" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/stephanie-weiss-4a2b3422b/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  





    <div class="project-related-pages content-widget-hr">
      
      

      
      
      

      
      
      
        <h2>Publications</h2>
        
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/conference-paper/" >An example conference paper</a>
    </div>

    
    <a href="/publication/conference-paper/"  class="summary-link">
      <div class="article-style">
        Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span class="author-highlighted">
      <a href="/author/stephanie-weiss/">Stephanie Weiss</a></span>, <span >
      <a href="/author/alexander-vossen/">Alexander Vossen</a></span>
      </div>
      
    </div>

    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/publication/conference-paper/" >
      <img src="/publication/conference-paper/featured_hu3d03a01dcc18bc5be0e67db3d8d209a6_312700_150x0_resize_q75_h2_lanczos.webp" height="100" width="150"
           alt="An example conference paper" loading="lazy">
    </a>
    
  </div>
</div>

        
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/preprint/" >An example preprint / working paper</a>
    </div>

    
    <a href="/publication/preprint/"  class="summary-link">
      <div class="article-style">
        Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span class="author-highlighted">
      <a href="/author/stephanie-weiss/">Stephanie Weiss</a></span>
      </div>
      
    </div>

    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/publication/preprint/" >
      <img src="/publication/preprint/featured_hu559a5add5185b02575aa8333502ab2cc_220813_150x0_resize_q75_h2_lanczos.webp" height="100" width="150"
           alt="An example preprint / working paper" loading="lazy">
    </a>
    
  </div>
</div>

        
      

      
      
      
        <h2>Events</h2>
        
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/talk/example-talk/" >Example Talk</a>
    </div>

    
    <a href="/talk/example-talk/"  class="summary-link">
      <div class="article-style">
        An example talk using Wowchemy&rsquo;s Markdown slides feature.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      
      <div>
        <span>
          Jun 1, 2030 1:00 PM &mdash; 3:00 PM
        </span>
        
        <span class="middot-divider"></span>
        <span>Wowchemy HQ</span>
        
      </div>
      

      
    </div>

    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/talk/example-talk/" >
      <img src="/talk/example-talk/featured_hu3d03a01dcc18bc5be0e67db3d8d209a6_620088_150x0_resize_q75_h2_lanczos.webp" height="107" width="150"
           alt="Example Talk" loading="lazy">
    </a>
    
  </div>
</div>

        
      
    </div>
  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy/Datenschutz</a>
    
    
       &middot; 
      <a href="/terms/">Imprint/Impressum</a>
    
  </p>
  

  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.3d946de2e8784a477845261d87025092.js"></script>

    
    
    
      
      
        <script src="https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js" integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/python.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    
      <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="/js/wowchemy-headroom.208bf5db800f4a4e5a38cf3b67a99a51.js" type="module"></script>
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.63639926e24ecfb8040ea052c1077969.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
